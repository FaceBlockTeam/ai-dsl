%% Technical Report for the work on the AI-DSL over the period of
%% March to May 2021.

\documentclass[]{report}
\usepackage{url}
\usepackage{minted}
\usepackage[textsize=footnotesize]{todonotes}
\newcommand{\kabir}[2][]{\todo[color=yellow,author=kabir, #1]{#2}}
\newcommand{\nil}[2][]{\todo[color=purple,author=nil, #1]{#2}}
\usepackage[hyperindex,breaklinks]{hyperref}
\usepackage{breakurl}
\usepackage{listings}
\lstset{basicstyle=\ttfamily\footnotesize,breaklines=false,frame=single}
\usepackage{float}
\restylefloat{table}
\usepackage{longtable}
\usepackage{graphicx}
\usepackage[font=small,labelfont=bf]{caption}
\usepackage[skip=0pt]{subcaption}

\begin{document}

\title{AI-DSL Technical Report (February to May 2021)}
\author{Nil Geisweiller, Kabir Veitas, Eman Shemsu Asfaw, Samuel Roberti}
\maketitle

\begin{abstract}
Based on~\cite{GoertzelGeisweillerBlog}.
\end{abstract}

\tableofcontents

\chapter{Nil's work}

Work done:
\begin{enumerate}
\item Implement \texttt{RealizedFunction} as described
  in~\cite{GoertzelGeisweillerBlog}.
\item Implement a network of trivially simple AI services implemented
  in Idris2, and use Idris compiler to type check if they can properly
  connect to each other.
\item Implement a Registry prototype, as a proof-of-concept for
  querying AI services based on their dependently typed
  specifications.
\end{enumerate}

\section{Realized Function}
\label{realized_function}

\subsection{Description}

The \texttt{RealizedFunction} data structure, as introduced
in~\cite{GoertzelGeisweillerBlog}, is a wrapper around a regular
function to integrate aspects of its specifications pertaining to its
execution on real physical substrates as opposed to just its
algorithmic properties.  For instance it contains descriptions of
costs (financial, computational, etc) and performances (quality, etc)
captured in the \texttt{RealizedAttributes} data structure, as
introduced in~\cite{GoertzelGeisweillerBlog} as well.

For that iteration we have implemented a simple version of
\texttt{RealizedFunction} and \texttt{RealizedAttributes} in
Idris2~\cite{Idris}.  The \texttt{RealizedAttributes} data structure
contains
\begin{itemize}
\item \texttt{Costs}: as a triple of three constants,
  \texttt{financial}, \texttt{temporal} and \texttt{computational},
\item \texttt{Quality}: as a single \texttt{quality} value.
\end{itemize}
as well as an example of compositional law,
\texttt{add\_costs\_min\_quality}, where costs are additive and
quality is infimum-itive.  Below is a small snippet of that code to
give an idea of how it looks like

\begin{minted}[mathescape]{idris}
record RealizedAttributes where
  constructor MkRealizedAttributes
  costs : Costs
  quality : Quality
\end{minted}

\begin{minted}[mathescape]{idris}
add_costs_min_quality : RealizedAttributes ->
                        RealizedAttributes ->
                        RealizedAttributes
add_costs_min_quality f_attrs g_attrs = fg_attrs where
  fg_attrs : RealizedAttributes
  fg_attrs = MkRealizedAttributes (add_costs f_attrs.costs g_attrs.costs)
                                  (min f_attrs.quality g_attrs.quality)
\end{minted}
The full implementation can be found in
\href{https://github.com/singnet/ai-dsl/blob/master/experimental/realized-function/RealizedAttributes.idr}{\texttt{RealizedAttributes.idr}},
under the
\href{https://github.com/singnet/ai-dsl/blob/master/experimental/realized-function/}{\texttt{experimental/realized-function/}}
folder of the \href{https://github.com/singnet/ai-dsl/}{AI-DSL
  repository}~\cite{AIDSLRepo}.\\

Then we have implemented \texttt{RealizedFunction} that essentially
attaches a \texttt{RealizedAttributes} instance to a function.  In
addition we have implemented a composition (as in function
composition) operating on \texttt{RealizedFunction} instead of
regular function, making use of that compositional law above.
Likewise below is a snippet of that code

\begin{minted}[mathescape]{idris}
data RealizedFunction : (t : Type) -> (attrs : RealizedAttributes) -> Type where
  MkRealizedFunction : (f : t) -> (attrs : RealizedAttributes) ->
                       RealizedFunction t attrs
\end{minted}

\begin{minted}[mathescape]{idris}
compose : {a : Type} -> {b : Type} -> {c : Type} ->
          (RealizedFunction (b -> c) g_attrs) ->
          (RealizedFunction (a -> b) f_attrs) ->
          (RealizedFunction (a -> c) (add_costs_min_quality f_attrs g_attrs))
compose (MkRealizedFunction g g_attrs) (MkRealizedFunction f f_attrs) =
  MkRealizedFunction (g . f) (add_costs_min_quality f_attrs g_attrs)
\end{minted}
The full implementation can be found in
\href{https://github.com/singnet/ai-dsl/blob/master/experimental/realized-function/RealizedFunction.idr}{\texttt{RealizedFunction.idr}}
under the same folder.

\subsection{Objectives and achievements}

The objectives of this work was to see if Idris2 was able to type
check that the realized attributes of composed realized functions
followed the defined compositional law.  We have found that Idris2 is
not only able to do that, but to our surprise much faster that Idris1
(instantaneous instead of seconds to minutes), by bypassing induction
on numbers and using efficient function-driven rewriting on the
realized attributes instead.  That experiment can be found in
\href{https://github.com/singnet/ai-dsl/blob/master/experimental/realized-function/RealizedFunction-test.idr}{\texttt{RealizedFunction-test.idr}},
under the
\href{https://github.com/singnet/ai-dsl/blob/master/experimental/realized-function/}{\texttt{experimental/realized-function/}}
folder of the \href{https://github.com/singnet/ai-dsl/}{AI-DSL
  repository}~\cite{AIDSLRepo}.

\subsection{Future work}

Experimenting with constants as realized attributes was the first step
in our investigation.  The subsequent steps will be to replace
constants by functions, probability distributions and other
sophisticated ways to represent costs and quality.

\section{Network of Idris AI services}
\label{network_idris_ai_services}

\subsection{Description}

In this work we have implemented a small network of trivially simple
AI services, with the objective of testing if the Idris compiler could
be used to type check the validity of their connections. Three primary
services were implemented
\begin{enumerate}
\item \texttt{incrementer}: increment an integer by 1
\item \texttt{twicer}: multiply an integer by 2
\item \texttt{halfer}: divide an integer by 2
\end{enumerate}
as well as composite services based on these primary services, such as
\begin{itemize}
\item \texttt{incrementer . halfer . twicer}
\end{itemize}
with the objective of testing that such compositions were properly
typed.  The networking part was implemented based on the
SingularityNET example service~\cite{SNETExampleService} mentioned in
the SingularityNET tutorial~\cite{SNETTutorial}.  The specifics of
that implementation are of little importance for that report and thus
are largely ignored.  The point was to try to be as close as possible
to real networking conditions.  For the part that matters to us we may
mention that communications between AI services are handled by
gRPC~\cite{gRPC}, which has some level of type checking by insuring
that the data being exchanged fulfill some type structures (list of
integers, union type of string and bool, etc) specified in Protocol
Buffers~\cite{Protobuf}.  Thus one may see the usage of Idris in that
context as adding an enhanced refined verification layer on top of
gRPC making use of the expressive power of dependent types.

\subsection{Objectives and achievements}

As mentioned above the objectives of such an experiment was to see how
the Idris compiler can be used to type check combinations of AI
services.  It was initially envisioned to make use of dependent types
by specifying that the \texttt{twicer} service outputs an even
integer, as opposed to any integer, and that the \texttt{halfer}
service only accepts an even integer as well.  The idea was to
prohibit certain combinations such as
\begin{itemize}
\item \texttt{halfer . incrementer . twicer}
\end{itemize}
Since the output of \texttt{incrementer . twicer} is provably odd,
\texttt{halfer} should refuse it and such combination should be
rejected.  This objective was not reached in this experiment, but
similar objectives were reached other experiments, see Section
\ref{aidsl_registry}\nil{Add ref to Sam's work}.  The other objective
was to type check that the compositions have realized attributes
corresponding to the compositional law implemented in Section
\ref{realized_function}, which was fully achieved in this experiment.
For instance by changing either the types, costs or quality of the
following composition
\begin{minted}[mathescape]{idris}
-- Realized (twicer . incrementer).
rlz_compo1_attrs : RealizedAttributes
rlz_compo1_attrs = MkRealizedAttributes (MkCosts 300 30 3) 0.9
-- The following does not work because 301 /= 200+100
-- rlz_compo1_attrs = MkRealizedAttributes (MkCosts 301 30 3) 0.9
rlz_compo1 : RealizedFunction (Int -> Int) Compo1.rlz_compo1_attrs
rlz_compo1 = compose rlz_twicer rlz_incrementer
\end{minted}
defined in
\href{https://github.com/singnet/ai-dsl/blob/master/experimental/simple-idris-services/service/Compo1.idr}{\texttt{experimental/simple-idris-services/service/Compo1.idr}},
the corresponding service would raise a type checking error at start
up.  More details on the experiment and how to run it can be found in
the
\href{https://github.com/singnet/ai-dsl/blob/master/experimental/simple-idris-services/README.md}{\texttt{README.md}}
under the
\href{https://github.com/singnet/ai-dsl/blob/master/experimental/simple-idris-services/}{\texttt{experimental/simple-idris-services/service/}}
folder of the \href{https://github.com/singnet/ai-dsl/}{AI-DSL
  repository}~\cite{AIDSLRepo}.

\subsection{Future work}

Such experiment was good to explore how Idris can be integrated to a
network of services.  What we need to do next is experiment with
actual AI algorithms, ideally making full use of dependent types in
their specifications.  Such endeavor was actually attempted over the
AI service collective described in Section
\ref{domain_model_considerations}, but it was eventually concluded to
be too ambitious for that iteration and was postponed for the next.

An intermediary step, before moving to actual AI algorithms, is to
explore multi-facet specifications and their interactions with other
AI services.  That is, a service A might output a data type satisfying
a combination of properties, such as, to tie that back to the trivial
incrementer, twicer and halfer combination, an integer that is both
even and within a certain interval.  Then two other services, B and C,
may have partial constraints instead.  For instance B may require an
even integer, while C may require that integer to be within a certain
interval.  In other words both can take as input the output of A, for
different reasons.  It's obviously not conceptual difficult to cast
the output of A to match the input types of B and C, however it is
still something we need to explore in practice with Idris.

Obviously we want to be able to reuse existing AI services and write
their enhanced specifications on top of them, as opposed to writing
both specification and code in Idris/AI-DSL.  To that end it was noted
that having a Protobuf to/from Idris/AI-DSL converter would be useful,
so that a developer can start from an existing AI service, specified
in Protobuf, and enriched it with dependent types in Idris/AI-DSL.
The other way around could be useful as well to enable a developer to
implement AI services entirely in Idris/AI-DSL and expose their
Protobuf specification to the network.  To that end having an
implementation of gRPC for Idris/AI-DSL could be handy as well.

\section{AI-DSL Registry}
\label{aidsl_registry}

\subsection{Description}

One important goal of the AI-DSL is to have a system that can perform
autonomous matching and composition of AI services, so that provided
the specification of an AI, it should suffice to find it, complete it
or even entirely build it from scratch.  We have implemented a
proof-of-concept \emph{registry} to start experimenting with such
functionalities.

So far we have two versions in the
\href{https://github.com/singnet/ai-dsl/}{AI-DSL repository}, one
without dependent types support, under
\href{https://github.com/singnet/ai-dsl/blob/master/experimental/registry/}{\texttt{experimental/registry/}},
and a more recent one with dependent type support that can be found
under
\href{https://github.com/singnet/ai-dsl/blob/master/experimental/registry-dtl/}{\texttt{experimental/registry-dtl/}}.
We will focus our attention on the latter which is far more
interesting.\\

The AI-DSL registry (reminiscent of the SingularityNET
registry~\cite{SNETRegistry}) is itself an AI service with the following functions
\begin{enumerate}
\item \texttt{retrieve}: find AI services on the network fulfilling a
  given specification.
\item \texttt{compose}: construct composite services fulfilling that
  specification.  Useful when no such AI services can be found.
\end{enumerate}

The experiment contains the same \texttt{incrementer}, \texttt{twicer}
and \texttt{halfer} services described in Section
\ref{network_idris_ai_services} with the important distinction that
their specifications now utilize dependent types.  For instance the
type signature of \texttt{twicer} becomes
\begin{minted}[mathescape]{idris}
twicer : Integer -> EvenInteger
\end{minted}
instead of
\begin{minted}[mathescape]{idris}
twicer : Integer -> Integer
\end{minted}
where \texttt{EvenInteger} is actually a shorthand for the following
dependent type
\begin{minted}[mathescape]{idris}
EvenInteger : Type
EvenInteger = (n : WFInt ** Parity n 2)
\end{minted}
that is a \emph{dependent pair} composed of a \emph{well founded
integer} of type \texttt{WFInt} and a dependent data structure,
\texttt{Parity} containing a proof that the first element of the pair,
\texttt{n}, is even.  More details on that can be found in
Section\nil{Add ref to Sam's work}.

For now our prototype of AI-DSL registry implements the
\texttt{retreive} function, which, given an Idris type signature,
searches through a database of AI services and returns one fulfilling
that type.  In that experiment the database of AI services is composed
of \texttt{incrementer}, \texttt{twicer}, \texttt{halfer}, the
\texttt{registry} itself and \texttt{compo}, a composite service using
previously listed services.

One can query each service via gRPC.  For instance querying the
\texttt{retreive} function of the \texttt{registry} service with the
following input
\begin{minted}[mathescape]{idris}
String -> (String, String)
\end{minted}
outputs
\begin{minted}[mathescape]{idris}
Registry.retreive
\end{minted}
which is itself.  Likewise one can query
\begin{minted}[mathescape]{idris}
Integer -> Integer
\end{minted}
which outputs
\begin{minted}[mathescape]{idris}
Incrementer.incrementer
\end{minted}
corresponding to the \texttt{Incrementer} service with the
\texttt{incrementer} function.
Next one can provide a query involving dependent types, such as
\begin{minted}[mathescape]{idris}
Integer -> EvenInteger
\end{minted}
outputting
\begin{minted}[mathescape]{idris}
Twicer.twicer
\end{minted}
Or equivalently provide the unwrapped dependent type signature
\begin{minted}[mathescape]{idris}
Integer -> (n : WFInt ** Parity n (Nat 2))
\end{minted}
retrieving the correct service again
\begin{minted}[mathescape]{idris}
Twicer.twicer
\end{minted}

At the heart of it is Idris.  Behind the scene the registry
communicates the type signature to the Idris REPL and requests, via
the {\texttt :search} meta function, all loaded functions matching the
type signature.  Then the registry just returns the first match.

Secondly, we can now write composite services with missing parts.  The
\texttt{compo} service illustrates this.  This service essentially
implements the following composition
\begin{minted}[mathescape]{idris}
incrementer . halfer . (Registry.retrieve ?type)
\end{minted}
Thus upon execution queries the registry to fill the hole with the
correct, according to its specification, service.

More details about this, including steps to reproduce it all, can be
found in the
\href{https://github.com/singnet/ai-dsl/blob/master/experimental/registry-dsl/README.md}{\texttt{README.md}}
under the
\href{https://github.com/singnet/ai-dsl/blob/master/experimental/registry-dsl/}{\texttt{experimental/simple-idris-services/service/}}
folder of the \href{https://github.com/singnet/ai-dsl/}{AI-DSL
  repository}~\cite{AIDSLRepo}.

\subsection{Objectives and achievements}

As shown above we were able to implement a proof-of-concept of an
AI-DSL registry.  Only the \texttt{retrieve} function was implemented.
The \texttt{compose} function still remains to be implemented,
although the \texttt{compo} service is somewhat halfway there, with
limitations, for instance the missing type, \texttt{?type}, was
hardwired in the code, \texttt{Integer -> EvenInteger}.  It should be
noted however that Idris is in principle capable of inferring such
information but more work is needed to more fully explore that
functionality.

Of course it is a very simple example, in fact the simplest we could
come up with, but we believe serves as a proof-of-concept, and
demonstrates that AI services matching, using dependent types as
formal specification language, is possible.

\subsection{Future work}

There a lot of possible future improvements for this work, in no
particular order
\begin{itemize}
\item Use structured data structures to represent type signatures
  instead of String.
\item Return a list of services instead of the first one.
\item Implement \texttt{compose} for autonomous composition.
\item Use real AI services instead of trivially simple ones.
\end{itemize}

Also, as of right now, the registry was implemented in
Python\footnote{because the SingularityNET example it is derived from
is written in Python, not because Python is thought to be the greatest
language for this purpose.}, querying Idris when necessary.  However
it is likely that this should be better suited to Idris itself.  Which
leads us to an interesting possibility, maybe the registry, and in
fact most (perhaps all) components and functions of the AI-DSL could
or should be implemented in the AI-DSL itself.

\chapter{AI-DSL Ontology (Kabir's work)}

\section{Description}

\subsection{Design requirements}
\label{design-requirements}

At the beginning of the current iteration of the AI-DSL project we had a round
of discussions about the high level functional and design requirements for
AI-DSL and its role in SingularityNET platform and ecosystem. The discussions
were based on
\cite{GoertzelGeisweillerBlog,singularitynet_foundation_phasetwo_2021}  and are
\href{https://github.com/nunet-io/ai-dsl-ontology/wiki/AI-DSL\%20requirements}{available
online} in their original form. Here is the summary of the preliminary design
requirements informed by those discussions:

\begin{itemize} \item AI-DSL is a language that allows AI agents/services
running on SinglarityNET platform to declare their capabilities and needs for
data to other AI agents in a rich and versatile machine readable form; This will
enable different AI agents to search, find data sources and other AI services
without human interaction; \item AI-DSL ontology defines data and service
(task) types to be used by AI-DSL. Requirements for the ontology are shaped by
the scope and specification of the AI-DSL itself; \end{itemize}

High level requirements for AI-DSL are:

\begin{description} \item[Extendability] The ontology of data types and AI task
types should be extendable in the sense that individual service providers /
users should be able to create new types and tasks and make them available to
the network. AI-DSL should be able to ingest these new types / tasks and
immediately be able to do the type-checking job. In other words, AI-DSL ontology
of types / tasks should be able to evolve. At the same time, extended ontologies
should relate to existing basic AI-DSL ontology in a clear way, allowing AI
agents to perform reasoning across the whole space of available ontologies
(which, at lower levels, may be globally inconsistent). In order to ensure
interoperability of lower level ontologies, AI-DSL ontology will define small
kernel / vocabulary of globally accessible grounded types, which will be
built-in into the platform at the deep level. Changing this kernel will most
probably require some form of voting / global consensus on a platform level.

  Therefore, it seems best to define AI-DSL Ontology and the mechanism of using
  it on two levels: \begin{itemize} \item \textit{The globally accessible
  vocabulary/root ontology of grounded types}. This vocabulary can be seen as
  immutable (in short and medium term) kernel. It should be extendable in the
  long term, but the mechanisms of changing and extending it will be quite
  complex, most probably involving theoretical considerations and/or a strict
  procedures of reaching global consensus within the whole platform (a sort of
  voting); \item \textit{A decentralized ontology of types and tasks} which each
  are based (i.e. type-dependent) on the root ontology/vocabulary, but can be
  extended in a decentralized manner -- in the sense that each agent in the
  platform will be able to define, use and share derived types and task
  definitions at its own discretion without the need of global consensus.
  \end{itemize}

  \item[Competing versions and consensus.] We want both consistency (for
  enabling deterministic type checking -- as much as it is possible) and
  flexibility (for enabling adaptation and support for innovation). This will be
  achieved by enforcing different restrictions for competing versions and
  consensus reaching on the two levels of ontology descrbed above:

  \begin{itemize} \item The globally accessible vocabulary / root ontology of
  grounded types will not allow for competing versions. In a sense, this level
  will be the true ontology, representable by a one and unique root /
  upper-level ontology of the network which users will not be able to modify
  directly; \item All other types and task definitions within the platform will
  be required to be derived from the root ontology (if they will want to be used
  for interaction with other agents); However, the platform whould not restrict
  the number of competing versions or define a global consensus  of types and
  task descriptions on this level. \item Furthermore, the ontology and the
  AI-DSL logic should allow for some variant of 'soft matching' which would
  allow to find the type / service that does not satisfy all requirements
  exactly, but comes as closely as available in the platform. \item At the
  lowest level of describing each instance of AI service or data source on the
  platform, AI-DSL shall allow maximum extendability in so that AI service
  providers and data providers will be able to describe and declare their
  services in the most flexible and unconstrained manner, facilitating
  competition and cooperation between them. \end{itemize}

  \item[Code-level / service-level APIs.] It is important to ensure that the
  ontology is readable / writable by different components of the SingularityNET
  platform, at least between AI-DSL engine / data structures and each AI service
  separately. This is needed because some of the required descriptors of AI
  services will have to be dynamically calculated at the time of calling a
  service and will depend on the immediate context (e.g. price of service, a
  machine on which it is running, possibly reputation score, etc.). It is not
  clear at this point how much of this functionality will be possible (and
  practical) to implement on available dependently typed, ontology languages or
  even if it is possible to use single language. Even it if is possible to
  implement all AI-DSL purely on the current dependently typed language choice
  Idris, it will have to interface with the world, deal with in-deterministic
  input from network and mutable states -- operations that may fail in run-time
  no matter how careful type checking is done during compile time
  \cite{brady_resource-dependent_2015}.

  Defining and maintaining code-level and service-level APIs will first of all
  enable interfacing SingularityNET agents to AI-DSL and therefore between
  themselves.

  \item[Key AI Agents properties] We can distinguish two somewhat distinct (but
  yet interacting) levels of AI-DSL Ontology AI service description level and
  data description level. It seems that it may be best to start building the
  ontology from the service level, because data description language is even
  more open-ended than AI description language, which is already open enough.
  Initially, we may want to include into the description of each AI service at
  least these properties: \begin{itemize} \item Input and output data structures
  and types \item Financial cost of service \item Time of computation \item
  Computational resource cost \item Quality of results \end{itemize} Most
  probably it is possible to express and reason about this data with Idris. It
  is quite clear however, that in order to enable interaction with and between
  SingularityNET agents (and NuNet adapters) all above properties have to be
  made accessible outside Idris and therefore supported by the code-level /
  service-level APIs and the SingularityNET platform in general.

\end{description}

\subsection{Domain model considerations}
\label{domain_model_considerations}

In order to attend to all high level design requirements. All levels of AI-DSL
Ontology should be developed simultaneously, so that we could make sure that the
work is aligned with the function and role of AI-DSL within SingularityNET
platform and ecosystem. We therefore use the "AI/computer-scientific"
perspective to ontology and ontology building -- emphasizing \textit{what an
ontologoy is for} -- rather than the "philosophical perspecive" dealing with
\textit{the study of what there is in terms of basic categories}
\cite{gruber_translation_1993,sep-logic-ontology}. Therefore we first propose
the  mechanism of how different levels (upper, domain and the leaf- (or
service)) of AI-DSL ontology will relate for facilitating interactions between
AI services on the platform.

Note, that design principles of such mechanism relate to the question how
abstract and consistent should relate to concrete and possibly inconsistent --
something that may need a deeper conceptual understanding than is attempted
during the project and presented here. We proceed in most practical manner for
proposing the AI-DSL ontology prototype, being aware that it may need to (and
possibly should) be subjected to more conceptual treatment in the future.

For a concrete domain model of AI-DSL ontology prototype we use the
\textit{Fake News
Warning}\footnote{\href{https://gitlab.com/nunet/fake-news-detection}{https://gitlab.com/nunet/fake-news-detection}}
application being developed by NuNet -- a currently incubated spinoff of
SingularityNET\footnote{\href{https://nunet.io}{https://nunet.io}}.

NuNet is the platform enabling dynamic deployment and up/down-scaling of
SingularityNET AI Services on decentralized hardware devices of potentially any
type. Importantly for the AI-DSL project, service discovery on NuNet is designed
in a way that enables dynamic construction of application-specific service
meshes from several SingularityNET AI services\cite{nunet_nunet_2021}. In order
for the service mesh to be deployed, NuNet needs only a specification of program
graph of the application. Note, that conceptually, construction of an
application from several independent containers is almost equivalent to
functionality explained in section~\ref{aidsl_registry} on AI-DSL Registry,
namely performance of matching and composition of AI services. This is the main
reason why we chose \textit{Fake News Warning} application as a domain model
for early development efforts of AI-DSL. However, we use this domain model
solely for the application-independent design of AI-DSL and attend to
its application specific aspects only as much as it informs the project.

The idea of dynamic service discovery is to enable application developers to
construct working applications (or at least their back-ends) by simply passing a
declarative definition of program graph to the special platform component
("network orchestrator") -- which then searches for appropriate SingularityNET
AI containers and connects them in to a single workflow (or workflows).
Suppose, that the back-end of \textit{Fake News Warning} app consists of three
SingularityNET AI containers \textit{news\_score}, \textit{uclnlp} and
\textit{binary-classification}:

\begin{table}[H]
  \scriptsize
  \centering
  \begin{tabular}{p{0.15\linewidth}|p{0.2\linewidth}|p{0.2\linewidth}|p{0.2\linewidth}|p{0.1\linewidth}}
    \textbf{Leaf item} & \textbf{Description} & \textbf{Input} &
    \textbf{Output} &
    \textbf{Source}\\
    \hline
    binary-classification & A pretrained binary classification model &
    English text of any length & 1 -- the text is categorized
    as fake; 0 -- text is categorized as not-fake & \textcopyright
    \href{https://gitlab.com/nunet/fake-news-detection/binary-classification}{NuNet}
    2021\\
    \hline
    uclnlp & Forked and adapted component of stance detection
    algorithm (\href{http://www.fakenewschallenge.org/#fnc1results}{FNC} third place
    winner) & Article title and text & Probabilities of the title \textit{agreeing},
    \textit{disagreeing}, \textit{discussing} or being \textit{unrelated} to the
    text & \textcopyright \href{https://github.com/uclnlp/fakenewschallenge}{UCL
    Machine Reading} 2017; \textcopyright
    \href{https://gitlab.com/nunet/fake-news-detection/uclnlp}{NuNet} 2021\\ \hline
    news-score & Calls dependent services, calculates overall result and sends them
    to the front-end & URL of the content to be checked & Probability that the content
    in the URL is fake & \textcopyright
    \href{https://gitlab.com/nunet/fake-news-detection/fake_news_score}{NuNet} 2021 \\
    \end{tabular}
  \captionsetup{width=0.7\linewidth}
  \caption{\label{tbl:fns_components}Description of each component of
  \texttt{Fake News Warning} application.}
\end{table}

Each component of application's back-end is a SingularityNET AI Service
registered on the platform.  Note, that as SingularityNET AI services are
defined through their specification and their
metadata\cite{SNETDocumentationServiceSetup}. The main purpose of the AI-DSL
Ontology is to be able to describe SNet AI Services in a manner that would allow
them to search and match each other on the platform and compose into complex
workflows -- similarly to what is described in Section
\ref{network_idris_ai_services}. Here is a simple representation of the program
graph of \textit{Fake News Warning} app:

\begin{figure}[h]
  \centering
    \begin{minted}[linenos,tabsize=2,breaklines, fontsize=\small]{json}
      "dag": {
        "news-score" : ["uclnlp","binary-classification"]
        }
    \end{minted}
    \vspace{-0.3cm}
    \captionsetup{width=0.7\linewidth}
    \caption{\label{lst:dag}A directed acyclic graph (DAT) of
      \textit{Fake News Warning} app
      prototype\cite{NuNetFakeNewsWarningAppRepo}.  Meaning that
      \texttt{news-score} depends on \texttt{uclnlp} and
      \texttt{binary-classification}.}
\end{figure}

The schematic representation of the \textit{Fake News Warning} app deployed as a
result of processing the DAG is depicted below. The addition of NuNet platform
to SingularityNET service discovery is that each service may be deployed on
different hardware environments, sourced by NuNet. When the application back-end
is deployed, it can be accessed from the GUI interface, which in this case is a
Brave browser extension.

\begin{figure}[H]
  \begin{subfigure}[b]{0.50\textwidth}
    \centering
    \includegraphics[width=0.9\textwidth]{../../../ontology/images/fake_news_detector.png}
    \captionsetup{width=0.8\linewidth}
    \caption{Schema of dependencies between backend components of the application
    (SingularityNET AI services potentially running on different machines).}
    \label{fig:fake_news_detector_schema}
  \end{subfigure}
  \begin{subfigure}[b]{0.50\textwidth}
    \centering
    \includegraphics[width=0.5\textwidth]{../../../ontology/images/fake_news_detector_browser_extension.png}
    \captionsetup{width=0.8\linewidth}
    \caption{Brave browser extension which calls the backend of
    \textit{Fake News Warning} application on each invocation on new
    content displayed in browser tab.}
  \end{subfigure}
\end{figure}

We will use this application design principles as the domain model for the first
design of AI-DSL Ontology and the prototype.

\subsection{Ontology language and upper level ontology}

After discussing several choices of ontology languages and reusing existing
ontologies for designing AI-DSL ontology\footnote{See
\href{https://github.com/singnet/ai-dsl/discussions/18}{Reusing Existing
Ontologies} discussion on AI-DSL Github repository\cite{AIDSLRepo}}, we have opted to
use SUO-KIF as an ontology language~\cite{pease_standard_2009} and SUMO as an
upper-level ontology~\cite{NilesPease2001}. The main motivation for this choice
were the versatility of KIF/SUO-KIF (Knowledge Interchange Format) language,
which essentially allows to express First Order Logic (FOL) statements in a simple
text format in terms of lisp-like syntax. Due to that, KIF can be easily
converted to other formats\cite{kalibatiene_survey_2011}. Also, a conversion to
Atomese -- the OpenCog's language also employing a list-like syntax -- has been
successfully attempted in the past\footnote{See
the \href{https://github.com/opencog/external-tools/tree/master/SUMO_importer}{SUMO
  Importer} in the OpenCog External Tools
repository\cite{ExternalToolsRepo}}. SUMO and the related ontology design tools
\cite{pease_sigma_2001} provide a convenient way for starting to design AI-DSL
Ontology levels and their relations.

\section{Objectives and achievements}

\subsection{Decentralized ontology}

In order to satisfy the \textit{extendibility} requirement of ontology design,
we are proposing a notion and design of a \textit{decentralized ontology}, which
enables us to work with globally consistent and locally incosistent components
within the same mechanism of AI-DSL. Based on our design, the full ontology of
\textit{Fake News Warning} application is constructed from a number of separate
components, which operate at different level of decentralization. Table below
describes each of these components.

\begin{table}[H]
%describe each kif file / level of ontology / consistent / inconsistent;
  \scriptsize
  \centering
  \begin{tabular}{p{0.24\linewidth}|p{0.24\linewidth}|p{0.24\linewidth}|p{0.24\linewidth}|}
    \textbf{Component} &
    \textbf{Description} &
    \textbf{Dependencies} &
    \textbf{Extendability}\\
    \hline
    \href{https://github.com/singnet/ai-dsl/blob/master/ontology/NuNet.kif}{NuNet.kif} &
    Defines classes to be used for describing each hardware resource eligible
    for running SingularityNET AI Services via NuNet platform;  &
    Merge.kif, SingularityNET.kif [,..]&
    Limited: versioning mechanism controlled by NuNet (to be defined) \\
    \hline
    \href{https://github.com/singnet/ai-dsl/blob/master/ontology/
    SingulairtyNet.kif}{SingularityNet.kif} &
    Defines global classes and types to be used for describing each
    SingularityNET AI Service &
    ComputerInput.kif, Merge.kif [,..]&
    Limited: versioning mechanism controlled by SingularityNET (to be defined)\\
    \hline
    \href{https://github.com/singnet/ai-dsl/blob/master/ontology/
    FakeNewsScore.kif}{FakeNewsScore.kif} &
    SingularityNET service responsible for constructing the whole back-end of
    each \textit{Fake News Warning} application instance i.e. program graph
    (DAG) of the application.&
    SingularityNET.kif [,..] &
    Fully decentralized: defined by application developers; Since \textit{Fake News
Warning} application is open source, any developer can for it and define it
otherwise; Technically, this would be a different application.\\
    \hline
    \href{https://github.com/singnet/ai-dsl/blob/master/ontology/fnsBinaryClassifier.kif}{fnsBinaryClassifier.kif} &
    A pre-trained binary classification model for fake news detection &
    SingularityNET.kif [,..] &
    Fully decentralized: defined by each algorithm developer independently.
    Technically, from the platform perspective, these will be different
    algorithms. \\
    \hline
    \href{https://github.com/singnet/ai-dsl/blob/master/ontology/
    NuNetEnabledComputer.kif}{NuNetEnabledCompu-ter.kif} &
    Each NuNet enabled hardware resource will have to be described accordingly
    when on-boarded to NuNet platform &
    NuNet.kif &
    Fully decentralized: independently defined by the owner of a hardware
    resource \\
    \hline
    \href{https://github.com/singnet/ai-dsl/blob/master/ontology/
    uclnlp.kif}{uclnlp.kif} &
    Forked and adapted component of stance detection algorithm by UCL Machine Reading group &
    SingularityNET.kif [,..] &
    Fully decentralized: defined by each algorithm developer independently.
    Technically, from the platform perspective, these will be different
    algorithms \\
    \hline
    \href{https://github.com/ontologyportal/sumo/blob/master/Merge.kif}{Merge.kif} &
    SUMO structural ontology, base ontology, numerical functions, set/class theory, temporal concepts and mereotopology &
    None - root ontology &
    Centralized and globally enforced -- defined by \href{http://www.ontologyportal.org/}{ontologyportal.org} \\
    \end{tabular}
  \captionsetup{width=0.7\linewidth}
  \caption{\label{tbl:all_kif_files}Description of each component of the AI-DSL Ontology prototope and links to related KIF files.}
\end{table}

\subsection{Ontology prototype}

Using the ontology levels described in Table \ref{tbl:all_kif_files} and
referenced files, we prototyped the ontology of \textit{Fake News Warning}
application.
\kabir{include link to sigma server with the prototoype ontology
installed (when ready)}

\begin{table}[H]
  \scriptsize
  \centering
  \begin{tabular}{p{0.5\linewidth}|p{0.4\linewidth}|}
    \textbf{Architectural level} & \textbf{Class} \\
    \hline
    SingularityNET platform & SNetAIService, SNetAIServiceIO,
    SNetAIServiceMetadata\\
    \hline
    NuNet platform & NuNetEnabledSNetAIService, NuNetEnabledComputer\\
  \end{tabular}
  \captionsetup{width=0.9\linewidth}
  \caption{\label{tbl:custom_classes_prototype}Main classes defined in AI-DSL
  ontology prototype per level of the \texttt{Fake News Warning} application's
  stack. Classes defined in SUMO are not included.}
\end{table}

AI algorithms onboarded on the SNet platfrom are instances of
\texttt{SNetAIService} class of sublasses of it.
Services of \texttt{Fake News Warning} application are defined as follows:

\begin{figure}[H]
  \begin{subfigure}[b]{1\textwidth}
    \centering
    \inputminted[firstline=1, lastline=2, linenos,tabsize=2,breaklines, fontsize=\small]{scm}{../../../ontology/uclnlp.kif}
    \vspace{-0.3cm}
    \captionsetup{width=0.8\linewidth}
    \caption{Service description}
    \vspace{0.3cm}
  \end{subfigure}
  \begin{subfigure}[b]{1\textwidth}
    \centering
    \inputminted[firstline=4, lastline=8, linenos,tabsize=2,breaklines, fontsize=\small]{scm}{../../../ontology/uclnlp.kif}
    \vspace{-0.3cm}
    \captionsetup{width=0.8\linewidth}
    \caption{Descriptions of service input and output types.}
    \vspace{0.3cm}
  \end{subfigure}
  \begin{subfigure}[b]{1\textwidth}
    \centering
    \inputminted[firstline=10, lastline=26, linenos,tabsize=2,breaklines, fontsize=\small]{scm}{../../../ontology/uclnlp.kif}
    \vspace{-0.3cm}
    \captionsetup{width=0.8\linewidth}
    \caption{Definition of types and their dependencies.}
  \end{subfigure}
\caption{\label{fig:serviceDefinitionKif}SNet AI Service definition in KIF
(uclnlp and binary-classification services are described in this way).}
\end{figure}

Type definitions and their dependency definitions are actually the domain of
formal type-checking part of AI-DSL and Idris related research. However,
irrespectively of which language will be eventually chosen for AI-DSL, Figure
\ref{fig:serviceDefinitionKif} expresses that we can:
\begin{enumerate}
  \item define correct serviceInput and serviceOutput types (unique for each
  service);
  \item potentially provide proofs that if a service data of correct type is
  provided on input, then it will output correctly typed data;
  \item if the above is not possible (which may be the default option when actual
  service AI are not written in Idris):
  \begin{enumerate}
    \item check if input data is of correct type at run-time and refuse to start
    service if it is not;
    \item check if output data is of correct type before sending it to the
    caller and raise error if it is not so;
  \end{enumerate}
\end{enumerate}

\textit{FakeNewsScore} AI Service is special in that it calls
other dependent services (as described by program graph in Figure
\ref{lst:dag}) and combines their results. We can define the program graph in
terms of dependencies between services in KIF as follows:

\begin{figure}[H]
  \captionsetup{width=0.8\linewidth}
  \inputminted[firstline=1, lastline=9, linenos,tabsize=2,breaklines, fontsize=\small]{scm}{../../../ontology/FakeNewsScore.kif}
  \vspace{-0.3cm}
  \caption{\label{fig:serviceDependencies}Defining program graph as a formal ontology. This is equivalent to DAG of Figure \ref{lst:dag}.}
\end{figure}

Figure \ref{fig:serviceDependencies} demonstrates how a workflow of connected
SingularityNET AI services can be statically defined and proven to work at
compile time. However, we could go further and define dependencies as
\textit{subclasses} of services with the same input/output data types. In such case
any instantiation of the subclass would be able to dynamically compile into the
workflow. Therefore we would not need to describe concrete dependencies -- they
would be dynamically resolved at run-time by matching input and output types.

\begin{figure}[H]
  \captionsetup{width=0.8\linewidth}
  \inputminted[firstline=1, lastline=22, linenos,tabsize=2,breaklines, fontsize=\small]{scm}{../../../ontology/FakeNewsScoreDynamic.kif}
  \vspace{-0.3cm}
  \caption{\label{fig:fakeNewsScoreDynamic}Defining generic input types instead
  of concrete dependencies in a \textit{FakeNewsScoreDynamic} service.}
\end{figure}

Any AI service with output type matching input type of the
\textit{FakeNewsScoreDynamic} could be compiled into the workflow:

\begin{figure}[H]
  \captionsetup{width=0.8\linewidth}
  \inputminted[firstline=1, lastline=4, linenos,tabsize=2,breaklines, fontsize=\small]{scm}{../../../ontology/uclnlpDynamic.kif}
  \vspace{-0.3cm}
  \caption{\label{fig:uclnlpDynamicOne}Using static globally defined types of
  input and output data structures of matching services eligible for
  compilation into a workfow.}
\end{figure}

However, systems with dependent typing, like Idris, may allow to go even further
and to find out if composite types are composed of the same components and
primitive types -- and thus match them.

\begin{figure}[H]
  \captionsetup{width=0.8\linewidth}
  \inputminted[firstline=7, lastline=26, linenos,tabsize=2,breaklines, fontsize=\small]{scm}{../../../ontology/uclnlpDynamic.kif}
  \vspace{-0.3cm}
  \caption{\label{fig:uclnlpDynamicTwo}Hypothetical usage of dynamic typing
  (most probably could be achieved in Idris, but not in KIF).}
\end{figure}

Primitive (or grounded) types (like \texttt{RealNumeber} and \texttt{Text} in
Figure \ref{fig:uclnlpDynamicTwo}), however, should be globally accessible and
unambiguously defined for this scheme to work.

All services of \textit{Fake News Warning} application are instances of
\texttt{NuNetEnablesSNetAIService} subclass, which, in turn, is a subclass of
\texttt{SNetAIService} class:
\begin{figure}[H]
  \begin{subfigure}[t]{1\textwidth}
    \centering
    \begin{minted}[linenos,tabsize=2,breaklines,fontsize=\small]{scm}
(instance fakeNewsScore NuNetEnabledSNetAIService)
(instance uclnlp NuNetEnabledSNetAIService)
    \end{minted}
    \vspace{-0.3cm}
    \captionsetup{width=0.8\linewidth}
    \caption{Declaration of \texttt{FakeNewsScore} service in
    \href{https://github.com/singnet/ai-dsl/blob/master/ontology/
    FakeNewsScore.kif}{FakeNewsScore.kif} and of \texttt{uclnlp} service
    in \href{https://github.com/singnet/ai-dsl/blob/master/ontology/uclnlp.kif}{uclnlp.kif}.}
    \vspace{0.3cm}
  \end{subfigure}
  \begin{subfigure}[t]{1\textwidth}
    \centering
    \inputminted[firstline=1, lastline=2, linenos,tabsize=2,breaklines, fontsize=\small]{scm}{../../../ontology/NuNet.kif}
    \vspace{-0.3cm}
    \captionsetup{width=0.8\linewidth}
    \caption{Definition of \texttt{NuNetEnabledSNetAIService} in \href{https://github.com/singnet/ai-dsl/blob/master/ontology/NuNet.kif}{NuNet.kif}.}
  \end{subfigure}
  \captionsetup{width=0.9\linewidth}
  \caption{Relation between SingularityNet and NuNet domain ontologies.}
  \label{fig:SNET_and_NuNet}
\end{figure}

Figure \ref{fig:SNET_and_NuNet} describes relation between
SingularityNET and NuNet platforms. \texttt{SNetAIService} class, defined in
 \href{https://github.com/singnet/ai-dsl/blob/master/ontology/
SingularityNET.kif}{SingularityNET.kif}, contains all requirements for the
metadata of the service to be published on SingularityNET platform.
\texttt{NuNetEnabledSNetAIService} extends \texttt{SNetAIService} by adding
metadata that is needed for this service to be deployed via NuNet APIs:

\begin{figure}[H]
  \centering
  \inputminted[firstline=4, lastline=11, linenos,tabsize=2,breaklines,
  fontsize=\small]{scm}{../../../ontology/NuNet.kif}
  \captionsetup{width=1\linewidth}
  \vspace{-0.3cm}
  \caption{The definition of \texttt{NuNetEnabledSNetAIService} in
  \href{https://github.com/singnet/ai-dsl/blob/master/ontology/NuNet.kif}{NuNet.kif}
  requires a service to have compute resource (and possibly other) requirements
  included in service metadata. The idea is that without required metadata
  fields, a service would not pass validation allowing it to be deployed via
  NuNet. An arbitrary amount of requirements could be defined here.}
  \label{fig:NuNetEnabledAIService_metadata_requirements}
\end{figure}

\texttt{NuNetEnabledSNetAIService}s can be deployed only on
\texttt{NuNetEnabledComputer}s, which expose their available computing resources
in a manner that the ability to run a service is automatically checked
\textbf{before} a service is dynamically deployed on a computer and a service
call is actually issued to it (see Figure
\ref{fig:NuNetEnabledComputer_requirements}). This formally described relation
between SingularityNET and NuNet ontologies enables to prove at 'compile time'
that a service will have enough computational  resources to be executed. Recall,
that SingularityNET ontology alone enables to  prove that a service or a
collection of services will return correct results when  called with correct
inputs.

\begin{figure}[H]
  \centering
  \inputminted[firstline=13, lastline=31, linenos,tabsize=2,breaklines,
  fontsize=\small]{scm}{../../../ontology/NuNet.kif}
  \captionsetup{width=1\linewidth}
  \vspace{-0.3cm}
  \caption{The definition of \texttt{NuNetEnabledComputer} in
  \href{https://github.com/singnet/ai-dsl/blob/master/ontology/NuNet.kif}{NuNet.kif}
  requires available computing resources, computer type and operating system to
  be listed in the metadata.}
  \label{fig:NuNetEnabledComputer_requirements}
\end{figure}

A \texttt{SNetAIService} can only be deployed on \texttt{NuNetEnabledComputer}
if available resources on the computer are not less than compute requirements of
a service:

\begin{figure}[H]
  \centering
  \inputminted[firstline=38, lastline=47, linenos,tabsize=2,breaklines,
  fontsize=\small]{scm}{../../../ontology/NuNet.kif}
  \captionsetup{width=1\linewidth}
  \vspace{-0.3cm}
  \caption{Constraints on eligible match between \texttt{SNetAIService} and
  \texttt{NuNetEnabledComputer} defined in
  \href{https://github.com/singnet/ai-dsl/blob/master/ontology/NuNet.kif}{NuNet.kif}
  and required for deployment of a service.}
  \label{fig:service_deployment_requirements}
\end{figure}

\texttt{SNetAIService} and \texttt{NuNetEnabledSNetAIService} classes are positioned within the SUMO ontology as follows:

\begin{table}[H]
  \scriptsize
  \centering
  \begin{tabular}{p{0.2\linewidth}|p{0.7\linewidth}|p{0.1\linewidth}|}
    \textbf{Class, subclass or instance} &
    \textbf{Description} &
    \textbf{Where defined} \\
    \hline
    Entity &
    The universal class of individuals. This is the root node of the ontology. &
    \href{https://github.com/ontologyportal/sumo/blob/master/Merge.kif}{Merge.kif}\\
    \hline
    Abstract &
    Properties or qualities as distinguished from any particular embodiment of
    the properties/ qualities in a physical medium. Instances of Abstract can be
    said to exist in the same sense as mathematical objects such as sets and
    relations, but they cannot exist at a particular place and time without some
    physical encoding or embodiment. &
    \href{https://github.com/ontologyportal/sumo/blob/master/Merge.kif}{Merge.kif}\\
    \hline
    Proposition &
    Propositions are Abstract entities that express a complete thought or a set of
such thoughts. Note that propositions are not restricted to the content
expressed by individual sentences of a Language. They may encompass the content
expressed by theories, books, and even whole libraries. A Proposition is a piece
of information, e.g. that the cat is on the mat, but a ContentBearingObject is
an Object that represents this information. A Proposition is an abstraction that
may have multiple representations: strings, sounds, icons, etc. For example, the
Proposition that the cat is on the mat is represented here as a string of
graphical characters displayed on a monitor and/ or printed on paper, but it can
be represented by a sequence of sounds or by some non-latin alphabet or by some
cryptographic form. &
    \href{https://github.com/ontologyportal/sumo/blob/master/Merge.kif}{Merge.kif}\\
    \hline
    Procedure &
    A sequence-dependent specification. Some examples are ComputerPrograms,
finite-state machines, cooking recipes, musical scores, conference schedules,
driving directions, and the scripts of plays and movies. &
    \href{https://github.com/ontologyportal/sumo/blob/master/Merge.kif}{Merge.kif}\\
    \hline
    ComputerProgram &
    A set of instructions in a computer programming language that can be
    executed by a computer. &
    \href{https://github.com/ontologyportal/sumo/blob/master/Merge.kif}{Merge.kif}\\
    \hline
    SoftwareContainer &
    &
    \href{https://github.com/singnet/ai-dsl/blob/master/ontology/
    SingulairtyNet.kif}{Singularity-Net.kif}\\
    \hline
    SNetAIService &
    Software package exposed via SNetPlatfrom and conforming to the special
    packaging rules &
    \href{https://github.com/singnet/ai-dsl/blob/master/ontology/    SingulairtyNet.kif}{Singularity-Net.kif}\\
    \hline
    NuNetEnabled-SNetAIService &
    SNetAIService which can be deployed on NuNetEnabledComputers and
    orchestrated via NuNet platfrom &
    \href{https://github.com/singnet/ai-dsl/blob/master/ontology/    NuNet.kif}{NuNet.kif}\\
    \hline
    \quad \textit{uclnlp} &
    Forked and adapted component of stance detection algorithm by UCL Machine
    Reading group. &
    \href{https://github.com/singnet/ai-dsl/blob/master/ontology/    uclnp.kif}{uclnlp.kif}
  \end{tabular}
\captionsetup{width=0.9\linewidth}
\caption{\label{tbl:uclnlp_hierarchy}Full hierarchy of dependencies of
\textit{uclnlp} SNet AI service instance within SUMO ontology. The same
hierarchy applies to binary-detection and FakeNewsScore services used
in the \textit{Fake News Warning} app.}
\end{table}

\kabir[inline]{Finished here 2021-06-03 03:55}

\subsection{The mechanism of dynamic workflow construction}

%Versions / voting / communication

\begin{enumerate}
  \item Upper level SUMO ontology
  (\href{https://github.com/ontologyportal/sumo/blob/master/Merge.kif}{Merge.kif});
  \item Middle level SUMO ontology
(\href{https://github.com/ontologyportal/sumo/blob/master/Mid-level-ontology.kif}{Mid-level-ontology.kif});
  \item Distributed computing hardware domain ontology in SUO-KIF
(\href{https://github.com/ontologyportal/sumo/blob/master/QoSontology.kif}{QoSontology.kif});
\end{enumerate}

\kabir[inline]{using ontology for agent communication in decentralized computing
systems, based on \cite{YvesHellenschmidt2002}}

\section{Future work}

\begin{itemize}
  \item In the long term, it may be ideal to develop a converter for converting
it to KIF, since OWL may be representable in KIF
\cite{martin_translations_nodate}  using \href{OWL
API}{https://github.com/owlcs/owlapi}; For the purpose of the ontology
prototype, we will manually select parts of the this ontology in order to build
the prototype and write them in SUO-KIF format;
  \item Similarly we want to be able to convert SUO-KIF specifications
    into Idris, and vise versa, to take advantage of the strengths of
    each formalism.  To the best of our knowledge there is no existing
    tools to automatically translating SUO-KIF to/from Idris, but
    there is a tool to translate SUO-KIF to
    FOL~\cite{Pease_firstorder} and we have found a paper describing
    the translation from a Dependently Typed Language (DTL) to
    FOL~\cite{SojakovaKristina2009}.  Additionally, to start building
    understanding about that, we have manually ported the trivial AI
    services described in Section \ref{aidsl_registry} to SUO-KIF, see
    \href{https://github.com/singnet/ai-dsl/blob/master/ontology/TrivialServices.kif}{TrivialServices.kif}
    under the
    \href{https://github.com/singnet/ai-dsl/blob/master/ontology}{ontology}
    of the \href{https://github.com/singnet/ai-dsl/}{AI-DSL
      repository}~\cite{AIDSLRepo}.  As it turns out writing formal
    specifications of functions in SUO-KIF is reasonably straight
    forward.  Here's the SUO-KIF implementation of the Twicer service
    to get a idea
\begin{minted}[mathescape]{idris}
(instance TwicerFn UnaryFunction)
(domain TwicerFn 1 Integer)
(range TwicerFn EvenInteger)
(=>
  (instance ?INTEGER Integer)
  (equal (TwicerFn ?INTEGER) (MultiplicationFn ?INTEGER 2))))
\end{minted}
where \texttt{EvenInteger} happens to be predefined in
\href{https://github.com/ontologyportal/sumo/blob/master/Merge.kif}{Merge.kif}
of SUMO, partially recalled below
\begin{minted}[mathescape]{idris}
(=>
  (instance ?NUMBER EvenInteger)
  (equal (RemainderFn ?NUMBER 2) 0))
\end{minted}
Thus one can see that is it easy to specify a function input type,
using \texttt{domain}, and output type, using \texttt{range}, in
SUO-KIF, as well its full or partial definition, using \texttt{=>},
\texttt{equal} and universally quantified variables such as
\texttt{?NUMBER}.  It should be noted however that the reason it works
so well in that case is because the output type does not depend on the
input value, the output is an even integer no matter what.  It is
expected that porting for instance the \texttt{append} function over
the dependent type \texttt{Vect}~\cite{Vectors} to SUO-KIF might not
be as trivial, since the \texttt{domain} and \texttt{range} constructs
cannot presumably represent such type dependence.  This however can be
moved to the function definition as offered by SUO-KIF expressiveness.
Another aspect we need to explore is how tools, such as Automatic
Theorem Provers (ATPs)~\cite{Baumgartner_automatedreasoning,
  Urban_anoverview, Alvez_evaluating_atp_adimen_SUMO}, can be used to
autonomously compose as well as retrieve functions given their input
and output types.  Obviously if ATP tools running over SUO-KIF turn
out to be deficient in that respect, we already know from Section
\ref{aidsl_registry} that Idris can fulfill that purpose.
\end{itemize}

\chapter{Eman's work}

\chapter{Software Engineering Strategies}

\section{Service Composition in Idris2}

A key requirement of the AI-DSL is to provide both an ergonomic syntax for
describing service properties and a robust process for using these descriptions
to verify the correctness of composed services.  This work involved
investigating several different methods for meeting this requirement using
Idris2.

\subsection{\texttt{RealizedFunction} and \texttt{RealizedAttributes}}
The \texttt{RealizedFunction} and \texttt{RealizedAttributes} data types were an
early strategy for describing and composing AI services.  They directly
contained values representing the relevant properties of arbitrary Idris
functions and made use of a \texttt{compose} function to compute the properties
of the function resulting from the composition of two others.

While this approach worked to verify that a small, fixed set of attributes was
correct for a composition of functions, it also presented several issues:

\begin{itemize}
  \item
        The \texttt{RealizedFunction} definition contains only the raw data
        representing function properties, while using a separate function to
        represent composition logic.  Because the composition logic is not
        part of the type definition, there is no way for Idris to prove that the
        correct logic was used to construct any given \texttt{RealizedFunction}.


  \item \texttt{RealizedAttributes} represents only a set of example properties.
        The syntax tree for the AI-DSL should be able to represent any
        properties specified by the user, assuming the composition laws for
        those properties are known.


\end{itemize}

\subsection{\texttt{Service}}
To address these problems, we implemented the \texttt{Service} type,
which can be found in experimental/realized-function/ServiceAttributes.idr.  It
differs from \texttt{RealizedFunction} in two important ways:
\begin{itemize}
  \item Composition logic is represented entirely at the type level as a second
        constructor for the \texttt{Service} type.

  \item Idris' \texttt{Num} interface is used as a generic representation of any
        attribute that can be added when two \texttt{Service}s are sequenced.

\end{itemize}

These changes were sufficient to solve the problems with our earlier approach,
but we still needed to improve the expressiveness of our representation.  Many
important properties are too complex to be described using only the \texttt{Num}
interface.

\subsection{A Look into Dependent Pairs}
Idris represents the intersection between a theorem proof assistant and a
programming language.  As such, it is often useful to think of types as
logical propositions, and values as proofs of those propositions.  Since our
goal is to verify that a desired property is true of some value, we can use
dependent types to describe a proposition parameterized by a specific value.

Idris provides a special syntax for this.
\texttt{(x : a ** p)} can be read as ``x is a value of type a such that
proposition p holds true of x''.  This is called a dependent pair, and it can
only be constructed by providing both a value and a proof that a desired
property holds true for that specific value.  In the context of service
composition, we can use dependent pairs as a direct representation of input
values that satisfy some condition.

To demonstrate the practicality of this pairing, consider the following types:

\begin{minted}[mathescape]{idris}

public export
data WFInt : Type where
     Nat : (n : Nat) -> WFInt
     Neg : (n : Nat) -> WFInt --Note: In the negative case, n=Z represents -1.

-- n-parity, i.e. proof that an integer a is evenly divisible by n (or not).
public export
data Parity : (a : WFInt) -> (n : WFInt) -> Type where
     -- a has even n-parity if there exists an integer multiple x s.t. x*n = a.
     Even : (x : WFInt ** (x * n) = a) -> Parity a n

public export
data OddParity : (a : WFInt) -> (n : WFInt) -> Type where
     -- a has odd n-parity if there exists
     Odd : (b : WFInt ** LT = compare (mag b) (mag n))
      -> (Parity (a + b) n) ->  OddParity a n

\end{minted}

\texttt{WFInt} is a type describing a well-founded view of an integer.  This
alternate view is necessary in order to write more flexible inductive proofs for
integer inputs.

\texttt{Parity} demonstrates the proof obligation necessary to
show that one integer is evenly divisible by another.  In plain English, it can
be read as ``If there exists some integer \texttt{x} such that
\texttt{x * n = a}, then \texttt{a} can be said to have \texttt{n-parity}.''
\texttt{OddParity} is a type representing the opposite proposition, i.e. that
dividing two integers will produce a remainder.

For services such as our Halfer example, this allows us to clearly express that
inputs should be only even numbers, as shown in this function type signature:

\begin{minted}[mathescape]{idris}
halfer : (a : WFInt ** Parity a 2) -> WFInt
\end{minted}

Similarly, the type signatures of the Twicer and Incrementer example services
can express their properties with regards to the 2-parity of the integers they
operate on:

\begin{minted}[mathescape]{idris}
-- Guaranteed to produce a value divisible by 2
twicer : (b : WFInt) -> (a : WFInt ** Parity a 2)

incrementer : (a : WFInt ** Parity a n) -> (b : WFInt ** OddParity b n)
\end{minted}

Now that the relevant properties for verification are expressed entirely at the
type level, the Idris2 typechecker can statically check the validity of service
compositions.

\begin{minted}[mathescape]{idris}

-- A valid sequence of services that successfully typechecks.
compo1 : WFInt -> WFInt
compo1 = fst (incrementer . halfer . twicer)

-- An invalid sequence of services that will always fail typechecking
compo2 : WFInt -> WFInt
compo2 = fst (halfer . incrementer . twicer)
\end{minted}

With dependent pairs, arbitrary properties of values can be encoded and formally
verified.  For an AI-DSL that may need to describe AI services in many different
contexts, this ability to use custom types instead of a limited set of
primitives is crucial.  However, this method is not a complete solution, as it
highlights major practical flaws.

An AI service developer making use of the AI-DSL should be able to adequately
describe the necessary properties of data their service will take as input, but
there should be no need for them to also encode the exact properties of their
service's output data.  A service developer is not likely to have any knowledge
of how their service's outputs will be used by other services in the future, so
the AI-DSL should not force them to describe their output data in any more
detail than is possible.  In the examples above, the \texttt{incrementer}
service was forced to describe its inputs and outputs in terms of properties
that are only relevant to other services.


\subsection{A Monadic DSL}

At this stage, there are two key problems which must be solved:

\begin{enumerate}

  \item At the point of service creation, developers should not be expected to
        have knowledge of the properties that are only relevant to other
        services.  They should be able to encode only the properties relevant to
        their own service.

  \item Due to the limits of computability, some relevant properties of data
        will not be formally provable.  However, some of these properties might
        still be safely assumed to hold in certain contexts, even if a formal
        proof is impossible.  The AI-DSL should be able to represent such cases
        and provide the strongest possible guarantees.

\end{enumerate}

For the first issue, we borrowed a well-established design pattern from
strongly-typed functional programming and defined a new \texttt{Service} type
around the \texttt{Monad} interface.  Monads are a class of types used to
describe a context for operations, along with any custom logic necessary to
combine those operations without imposing any requirement for tight coupling.
This is perfect for the AI-DSL.

To address the issue of unproveable properties, we experimented with a
conceptual model of smart contracts as a core language feature within the DSL.
Because the actual implementation of logic to represent external smart contracts
was outside the scope of this work, we made the assumption that such contracts
could be used to represent a financially-backed assurance that some unproveable
property holds.  In theory, this could allow compositions of AI services to be
analyzed for their overall financial risk.



\section{Depth of Embedding}

A domain-specific language requires not only a formal specification for its
semantics, but a software implementation as well.  The relationship between a
DSL and its implementation can vary, but most strategies fall into one or more
of three categories:

\begin{enumerate}
  \item \texttt{Independent Syntax:} A language may be designed completely
        separately from its implementation.  Such languages typically require
        dedicated compilers or interpreters, as they are unable to borrow any
        functionality due to their lack of a host language.

  \item \texttt{Shallow Embedding:} An embedded domain-specific language (eDSL)
        is written as a module or library for some host language.  Data in the
        DSL's domain is represented directly as values in the host language.
        Shallow embeddings tend to be easy to use and extend, but often suffer
        issues with performance and expressiveness.  Programs written in a
        shallowly-embedded DSL can only describe operations in the domain of
        their host language, and thus are limited to a single interpretation.

  \item \texttt{Deep Embedding:} Similarly to a shallowly-embedded DSL,
        an eDSL with a deep embedding is defined in some host language.
        Deep embeddings define a custom GADT in the host language and represent
        all data as values of this type.  Because the entire abstract syntax
        tree of a deeply-embedded program is a single type, it is simple to
        write functions in the host language that operate directly on the
        embedded program. This allows for automatic optimization of embedded
        programs, as well as multiple possible interpretations.  However, any
        extensions to a deep eDSL require significant effort, as changes to the
        language's AST type incur a requirement to update every function that
        operates on that type.

\end{enumerate}

For the AI-DSL, the most promising approach appears to be a hybrid method.
The basic domain of the DSL can be defined as deep embedding, while more
specialized features can be shallowly embedded as smaller DSLs within the main
AI-DSL instead of directly in Idris2.



\bibliographystyle{splncs04}
\bibliography{local}

\end{document}
