%% Technical Report for the work on the AI-DSL over the period of May
%% to September 2022.

\documentclass[]{report}
\usepackage{url}
\usepackage{minted}
\usepackage[textsize=footnotesize]{todonotes}
\newcommand{\kabir}[2][]{\todo[color=yellow,author=kabir, #1]{#2}}
\newcommand{\nil}[2][]{\todo[color=purple,author=nil, #1]{#2}}
\usepackage[hyperindex,breaklinks]{hyperref}
\usepackage{breakurl}
\usepackage{listings}
\lstset{basicstyle=\ttfamily\footnotesize,breaklines=false,frame=single}
\usepackage{float}
\restylefloat{table}
\usepackage{longtable}
\usepackage{graphicx}
\usepackage[font=small,labelfont=bf]{caption}
\usepackage[skip=0pt]{subcaption}
\usepackage{circledsteps}

\begin{document}

\title{AI-DSL Technical Report (May to Septembre 2022)}
\author{Nil Geisweiller, Samuel Roberti}
\maketitle

\begin{abstract}
\end{abstract}

\tableofcontents

\chapter{Introduction}

\section{Setting the Scene}

In the previous iteration we explored using Dependent Types to express
formal specifications of AI services, with the ultimate goal of
building a language for easily writing those specifications, the
AI-DSL itself, as well as services to automatically connect AI
services together, the AI-DSL Registry~\cite{AIDSLReport2021}.

Back then we experimented with trivial AI services, computing simple
arithmetic, described by trivial properties, such as the parity of
their inputs/outputs.  We were able to demonstrate that Idris, our DTL
of choice, could be used to verify the correctness of such AI service
assemblages.  The approach seemed promising, but to really put the
idea to the test we had to make progress on two fronts:

\begin{enumerate}
\item Replace trivial AI services by actual AI algorithms.
\item Explore program synthesis, as it became clear that it was at the
  heart of this endeavor.  First, for building the AI service
  assemblages themselves.  Second, for achieving fuzzy matching, that
  is when AI services almost fit together but not quite yet.  And
  third, for completing assemblages when some AI services are outright
  missing.
\end{enumerate}
That is what we have done during that iteration.

\section{Work Accomplished}

First we have implemented three AI algorithms in Idris:
\begin{enumerate}
\item Gradient descent
\item Linear regression
\item Logistic regression
\end{enumerate}
These algorithms were chosen because they are relatively simple, yet
extensively use in real world applications, as well as tightly related
to each other.  Linear regression can be framed as a gradient descent
problem, and logistic regression can be framed both as gradient
descent and linear regression problems, thus constituting an excellent
case study for the AI-DSL.  Alongside these implementations, a
descending property was formulated and formally proved for each
algorithm.

Finally, we have explored ways to perform program synthesis of
dependently typed programs.  While we have only achieved partial
success as far as program synthesis is concerned, we were able to
demonstrate its feasibility within the Idris ecosystem.  It was clear
from the start anyway that to be done well and fully, program
synthesis essentially requires achieving AGI.  Indeed, it is one of
these AI-complete problems.  That is any problem can be framed as a
program synthesis problem and vice versa.  The idea being that such
functionality can be progressively grown, deferred less and less to
human intervention, as the network and the AI-DSL evolve.

\section{Related Work}

Here's a list of projects and publications we have discovered along
the way that relate to the work done during that iteration.  TODO

\chapter{Implementation and Verification of AI Algorithms}

\section{Implementation of AI Algorithms}
We have implemented the following AI algorithms in Idris:
\begin{enumerate}
\item Descent: a generic descending algorithm.
\item Gradient Descent: a gradient descent algorithm using Descent.
\item Linear Regression: a linear regression algorithm using Gradient
  Descent.
\item Logistic Regression: a logistic regression algorithm using
  Gradient Descent.
\item Logistic-Linear Regression: a logistic regression algorithm
  using Linear Regression.
\end{enumerate}
\begin{figure}[H]
  \centering
  \includegraphics[scale=0.8]{figs/ai-algorithms.xfig.pdf}
  \caption{AI algorithms call graph}
  \label{fig:ai_algorithms}
\end{figure}
A call graph is provided in Figure~\ref{fig:ai_algorithms}.  Each
algorithm may be viewed as an AI service, together forming a network
of AI services delegating work to one another when possible.

The idea of performing logistic regression via two paths, either
directly via calling Gradient Descent, or indirectly via calling
Linear Regression, came from the ambitious goal of having our AI-DSL
prototype discover an alternate way, possibly unforeseen by the AI
practitioner, to perform a certain AI tasks, here logistic regression.
As we will see we did not come far enough to achieve that goal, but we
certainly keep on the side for the future.

Let us now describe in more details what these algorithms are doing,
and then provide the descending property we have focused on in this
work.

\subsection{Descent}
The Descent algorithm takes in input:
\begin{enumerate}
\item a cost function to minimize;
\item a step function to jump from candidate to candidate;
\item an initial candidate to start the search from;
\item a maximum number of steps allocated to the search;
\end{enumerate}
and outputs the final candidate as well as the remaining unallocated
steps.

\subsection{Gradient Descent}
The Gradient Descent algorithm takes in input:
\begin{enumerate}
\item a loss function;
\item a gradient function;
\item a learning rate, also called step size;
\item an initial candidate to start the search from;
\item a maximum number of steps allocated to the search;
\end{enumerate}
converts the gradient function and the learning rate into a step
function, calls the Descent algorithm and returns the final candidate
as well as the remaining unallocated steps.

\subsection{Linear Regression}
The Linear Regression algorithm takes in input:
\begin{enumerate}
\item a data set to explain, a matrix of inputs and a column vector
  of outputs;
\item a learning rate, also called step size;
\item an initial candidate to start the search from;
\item a maximum number of steps allocated to the search;
\end{enumerate}
defines a sum-of-squared-errors-based loss and gradient functions for
that data set, calls Gradient Descent and returns the final candidate
as well as the remaining unallocated steps.

\subsection{Logistic Regression}
The Logistic Regression algorithm takes in input:
\begin{enumerate}
\item a data set to explain, a matrix of inputs and a Boolean column
  vector of outputs;
\item a learning rate, also called step size;
\item an initial candidate to start the search from;
\item a maximum number of steps allocated to the search;
\end{enumerate}
defines a cross-entropy-based loss and gradient functions for that
data set, calls Gradient Descent and returns the final candidate as
well as the remaining unallocated steps.

\subsection{Logistic-Linear Regression}
The Logistic-Linear Regression algorithm takes in input:
\begin{enumerate}
\item a data set to explain, a Boolean matrix of inputs and a Boolean
  column vector of outputs;
\item a learning rate, also called step size;
\item an initial candidate to start the search from;
\item a maximum number of steps allocated to the search;
\end{enumerate}
transforms the data set so that the column vector of outputs
represents the odds of outputting True instead of a Boolean value,
calls linear regression on that transformed data set and returns the
final candidate as well as the remaining unallocated steps.

\section{Verification of AI Algorithms}

The concept of verifying properties of AI algorithms is a very broad
one, could be verifying the AI algorithms themselves, or their output
models, either using crisp mathematical properties, or empirical fuzzy
NEXT

\subsection{Descending Property for Descent}
Here we focus on the simplest one we could possibly imagine in this
situation, which is that the algorithm must descend, or at least not
ascend.  In other words, that the final candidate must be better, or
at least not worse, that the initial one.  This may seem like an
overly simplistic property, and it is.  However, as we will see,
working with that was already quite an educational journey.

Let us begin by showing the Idris implementation of Descent, or rather
a slightly simplified version modified for expository purpose:
\begin{minted}[mathescape]{idris}
descent : Ord cost_t =>
          (cnd_t -> cost_t) ->    -- Cost function
          (cnd_t -> cnd_t) ->     -- Step function
          (cnd_t, Nat) ->         -- Init candidate, steps
          (cnd_t, Nat)            -- Final candidate, steps
descent _ _ (cnd, Z) = (cnd, Z)
descent cost next (cnd, S k) = if cost (next cnd) < cost cnd
                               then descent cost next (next cnd, k)
                               else (cnd, (S k))
\end{minted}
It essentially expresses that if the cost of the next candidate is
less than the cost of the initial candidate, it should recursively
descend from the next candidate, otherwise return the initial
candidate.  Note that \texttt{cnd\_t} and \texttt{cost\_t} are type
variables, that is they may be substituted by any type, up to some
constraints, at function call.  The descending property can then be
formalized as follows:
\begin{minted}[mathescape]{idris}
descent_le : Ord cost_t =>
             (cost : cnd_t -> cost_t) -> -- Cost function
             (next : cnd_t -> cnd_t) ->  -- Step function
             (cas : (cnd_t, Nat)) ->     -- Init candidate, steps
  (cost (fst (descent cost next cas)) <= cost (fst cas)) === True
\end{minted}
which expresses that the cost of the final candidate should be less
than or equal to the cost of the initial candidate\footnote{For
  information, \texttt{===} denotes the equality type, a dependent
  type with \texttt{Refl} as sole constructor corresponding to the
  reflexivity axiom of equality.}.  Obviously such property should be
trivial to prove given how the algorithm has been written.  In
practice however, it is not so, for two reasons:
\begin{enumerate}
\item Idris makes no assumption about the comparison operators
  \texttt{<}, \texttt{>}, \texttt{<=} and \texttt{>=}.  The interface
  \texttt{Ord} guaranties that \texttt{cost\_t} implements these
  operators, but not how they should behave.  Thus one needs to encode
  these assumptions and make sure that they are true for the types of
  interest, which is not always easy, or even possible, especially for
  primitive types like \texttt{Double}.
\item Since the algorithm is recursive, it requires a recursive proof.
\end{enumerate}
To address the first reason we added a number of functions formalizing
the usual axioms of total strict and non-strict orders of \texttt{<},
\texttt{>}, \texttt{<=} and \texttt{>=}.  A small snippet is given
below:
\begin{minted}[mathescape]{idris}
||| Assume that < is irreflexive
lt_irreflexive : Ord a => {0 x : a} -> (x < x) === False
lt_irreflexive = believe_me ()

||| Assume that < is connected
lt_connected : Ord a => {0 x, y : a}
                     -> (x < y) === False
                     -> (y < x) === False
                     -> x === y
lt_connected _ _ = believe_me ()

||| Assume that <= is reflexive
le_reflexive : Ord a => {0 x : a} -> (x <= x) === True
le_reflexive = believe_me ()

||| Assume that <= is transitive
public export
le_transitive : Ord a => {0 x, y, z : a}
                      -> (x <= y) === True
                      -> (y <= z) === True
                      -> (x <= z) === True
le_transitive _ _ = believe_me ()
\end{minted}
The whole list of axioms can be found in
file~\href{https://github.com/singnet/ai-dsl/blob/master/experimental/ai-algorithms/descent/Search/OrdProofs.idr}{OrdProofs.idr}
of the~\href{https://github.com/singnet/ai-dsl}{ai-dsl} repository.
We also attempted to use an existing library from Stefan H\"ock
called~\href{https://github.com/stefan-hoeck/idris2-prim}{idris2-prim},
but decided to write our own for more flexibility.\\

The proof of \texttt{descent\_le}, slightly simplified to suit our
simplified version of \texttt{descent}, is presented below.  Let us
first deal with the base case where the number of allocated steps is
zero:
\begin{minted}[mathescape]{idris}
descent_le _ _ (_, Z) = le_reflexive
\end{minted}
In order to prove the descending property it suffices to invoke the
reflexivity of \texttt{<=} since for that case \texttt{descent} merely
becomes the identity function.  Let us now examine the recursive case
where the number of allocated steps is greater than zero:
\begin{minted}[mathescape]{idris}
descent_le cost next (cnd, S k)
           with ((cost (next cnd)) < (cost cnd)) proof eq
  _ | True = let des_le_nxtcst = descent_le cost next (next cnd, k)
                 nxtcst_le_cst = le_reflexive_closure_lt (Left eq)
              in le_transitive des_le_nxtcst nxtcst_le_cst
  _ | False = le_reflexive
\end{minted}
The proof considers the two branches of the conditional.  If the
condition is false then invoking the reflexivity of \texttt{<=}
suffices for the same reason as above.  If the condition is true then
the proof needs to combine axioms about comparison with the recursion
of \texttt{descent\_le} and the transitivity of \texttt{<=}.

That simplified proof is already somewhat substantial, likely too
substantial to be rapidly discovered by a greedy proof search
algorithm.  The non simplified version of Descent as well as the proof
of its descending property, about the double the size of the
simplified one, can be found in
file~\href{https://github.com/singnet/ai-dsl/blob/master/experimental/ai-algorithms/descent/Search/Descent.idr}{Descent.idr}
of the~\href{https://github.com/singnet/ai-dsl}{ai-dsl} repository.
Discovering such a proof automatically or semi-automatically still
remains relatively practical, either by requiring human intervention,
using proof tactics or more sophisticated inference control
techniques~\cite{Goertzel2014EGI2Chapt18}.

\subsection{Descending Property for Other Algorithms}
Once the descending property has been proved for Descent, proving it
for the remaining algorithms is now truly trivial, for the most part
anyway.

Let us provide an example for Gradient Descent, starting by recalling
what is the gradient descent algorithm.  Given a loss function $L$ and
a learning rate $\eta$, the gradient descent algorithm works by
updating the candidate $\beta$ as follows
$$\beta := \beta - \eta \nabla L(\beta)$$
in other words, the step function takes the opposite direction of the
gradient by a factor of $\eta$.  The Idris code of Gradient Descent is
given below:
\begin{minted}[mathescape]{idris}
gradientDescent : (Ord a, Neg a) =>
  (cost : ColVect m a -> a) ->          -- Cost function
  (grd : ColVect m a -> ColVect m a) -> -- Gradient
  (eta : a) ->                          -- Learning rate
  (cas : (ColVect m a, Nat)) ->         -- Init candidate, steps
  (ColVect m a, Nat)                    -- Final candidate, steps
gradientDescent cost grd eta = descent cost (fsgrd grd eta)
\end{minted}
where \texttt{fsgrd} is a function that takes a gradient,
\texttt{grd}, a learning rate, \texttt{eta}, and produces the step
function described above.  The type of a candidate for Gradient
Descent is now more specific.  Instead of being the variable type
\texttt{cnd\_t}, it is a column vector of size \texttt{m} and type
\texttt{a} represented by \texttt{ColVect m a}.

The descending property for Gradient Descent is expressed as follows:
\begin{minted}[mathescape]{idris}
gradientDescent_le : (Ord a, Neg a) =>
  (cost : ColVect m a -> a) ->           -- Cost function
  (grd : ColVect m a -> ColVect m a) ->  -- Gradient
  (eta : a) ->                           -- Step size
  (cas : (ColVect m a, Nat)) ->          -- Init candidate, steps
  (cost (fst (gradientDescent cost grd eta cas)) <= cost (fst cas))
   === True
\end{minted}
And its proof is simply
\begin{minted}[mathescape]{idris}
gradientDescent_le cost grd eta = descent_le cost (fsgrd grd eta)
\end{minted}
that is the proof of the descending property of Descent. Idris is able
to directly reuse it because it automatically applies the rule of
replacement in the type definition on the function calls present in it
by using their definitions.  So for instance
\begin{minted}[mathescape]{idris}
(cost (fst (gradientDescent cost grd eta cas)) <= cost (fst cas))
\end{minted}
is automatically replaced by
\begin{minted}[mathescape]{idris}
(cost (fst (descent cost (fsgrd grd eta) cas)) <= cost (fst cas))
\end{minted}
which is what \texttt{descent\_le} proves.

Proving the descending properties on the other algorithms, with the
exception of Logistic-Linear Regression, is equally trivial.  Proving
it for Logistic-Linear Regression requires an explicit use of the rule
of replacement.

\chapter{Program Synthesis}
\label{chap:program_synthesis}

\section{Language Framework}

\section{Idris Elaboration}

\section{Idris Proof Search}
Since recently, Idris2 has introduced a functionality called Proof
Search.  Contrary to what its name suggests however, it can be used
for program synthesis, not just proof search -- which should be no
surprise to those familiar with the Curry-Howard correspondence.  It
has however, at the time of writing this document, a number of
downsides.  The main one being it can only access
\begin{enumerate}
\item data type constructors,
\item variables in its current environments.
\end{enumerate}
Meaning, it does not have access to functions or constants defined in
the current and imported modules.  The other downsides are that it is
poorly documented and difficult to control, likely due to having being
introduced so recently.

Nonetheless, in this section we explore how such functionality can be
used for program synthesis in spite of its current limitations.

\subsection{Program Synthesis with Abstract Syntax Trees}
\label{subsec:AST}
The idea is to represent programs as Abstract Syntax Trees.  Each
operator can be represented as a constructor of that data structure of
that Abstract Syntax Tree, which Idris can access to generate trees
representing programs.  Here is a minimal example:
\begin{minted}[mathescape]{idris}
||| Abstract Syntax Tree Types
data Ty = TyDouble | TyCandidate | TyFun Ty Ty

||| Abstract Syntax Tree Terms
data Expr : Ty -> Type where
    Candidate : Expr TyCandidate
    Loss : Expr (TyFun TyCandidate TyDouble)
    Gradient : Expr (TyFun TyCandidate TyCandidate)
    Descent : Expr (TyFun TyCandidate TyDouble) ->
              Expr (TyFun TyCandidate TyCandidate) ->
              Expr TyCandidate ->
              Expr TyCandidate
\end{minted}
Then one can ask Idris to fill the hole of the following definition
\begin{minted}[mathescape]{idris}
linearRegression : Expr TyCandidate
linearRegression = ?hole
\end{minted}
which it successfully does by suggesting a number of candidates to
replace \texttt{?hole} by, such as
\begin{minted}[mathescape]{idris}
Candidate
Descent Loss Gradient Candidate
Descent Loss Gradient (Descent Loss Gradient Candidate)
...
\end{minted}
The second suggestion corresponds to implementation we are looking
for.

\subsection{Program Synthesis with Variables}
Let us now explore using environment variables to represent constant
and functions instead of constructors.  The meta-function \texttt{syn}
described below:
\begin{minted}[mathescape]{idris}
syn : (a -> b -> c) ->
      (a -> b -> c) ->
      (a -> b -> c) ->
      a -> b -> c
syn f g h x y = ?hole
\end{minted}
and takes 3 functions, \texttt{f}, \texttt{g} and \texttt{h}, as
arguments, and outputs a function that takes 2 arguments of types
\texttt{a} and \texttt{b} respectively.  Idris can successfully
attempt to can fill the hole by suggesting the following candidates
\begin{minted}[mathescape]{idris}
h x y
g x y
f x y
\end{minted}
which cover all possibilities in that instance.

Here is another example attempting to reproduce the one using Abstract
Syntax Trees provided in Section~\ref{subsec:AST}.
\begin{minted}[mathescape]{idris}
syn : (cnd -> cnd) ->                          -- Step function
      ((cnd -> cost) -> (cnd -> cnd) -> cnd -> cnd) -> -- Descent
      (cnd -> cost) ->                         -- Cost function
      cnd ->                                   -- Init candidate
      cnd                                      -- Final candidate
syn n d c i = ?hole
\end{minted}
Idris again finds the candidate we are looking for, that is the second
suggestion in the list below:
\begin{minted}[mathescape]{idris}
i
d c n i
d c n (d c n i)
...
\end{minted}

The full experiments can be found in folder~\href{https://github.com/singnet/ai-dsl/blob/master/experimental/program-synthesis/idris-proofsearch}{idris-proofsearch}
of the~\href{https://github.com/singnet/ai-dsl}{ai-dsl} repository,
and contain more attempts including unsuccessful ones using the
\texttt{let} keyword not covered here.  Of course these experiments
are both very simplistic and too unconstrained but the fact that they
work indicates that synthesizing programs, with more operators and
types, including dependent types representing properties, should be
possible with standalone Idris.  And as Idris Proof Search
functionality improves, it might even become a viable option in
practice.  Other options that would be worth exploring would be to
experiment with the Proof Search functionalities of other DTLs such as
AGDA and Coq.
\chapter{Conclusion}

Determinstic -> Stochastic
Crisp mathematical -> Empirical

\appendix
\chapter{Glossary}
\begin{itemize}
\item \textbf{AI service assemblage}: collection of AI services
  interacting together to fulfill a given function.  Example of such
  AI service assemblage would be the Nunet Fake News Warning system.
\item \textbf{Dependent Types}: types depending on values.  Instead of
  being limited to constants such as \texttt{Integer} or
  \texttt{String}, dependent types are essentially functions that take
  values and return types.  A dependent type is usually expressed as a
  term containing free variables.  An example of dependent type is
  \texttt{Vect n a}, representing the class of vectors containing
  \texttt{n} elements of type \texttt{a}.
\item \textbf{Dependently Typed Language}: functional programming
  language using dependent types.  Examples of such languages are
  Idris, AGDA and Coq.
\item \textbf{DTL}: Shorthand for Dependently Typed Language.
\end{itemize}

\bibliographystyle{splncs04}
\bibliography{local}

\end{document}
